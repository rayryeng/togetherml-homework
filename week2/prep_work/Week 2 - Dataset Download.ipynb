{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2 Assignment - Dataset Download\n",
    "\n",
    "For this homework, I will be downloading images of Star Trek: The Next Generation (TNG) characters - specifically those who belong in the main cast.  This is for the purpose of creating a deep neural network for classifying TNG characters.\n",
    "\n",
    "The characters are:\n",
    "\n",
    "* Capt. Jean-Luc Picard\n",
    "* Cmdr. William Riker\n",
    "* Lt. Cmdr. Geordi La Forge\n",
    "* Lt. Worf\n",
    "* Dr. Beverly Crusher\n",
    "* Counselor Deanna Troi\n",
    "* Lt. Cmdr. Data\n",
    "* Lt. Tasha Yar\n",
    "* Dr. Katherine Pulaski\n",
    "* Ens. Wesley Crusher\n",
    "\n",
    "To start off, we will be heavily borrowing from Jeremy Howard's `fast.ai` course: https://github.com/fastai/course-v3/blob/master/nbs/dl1/lesson2-download.ipynb by using some utility scripts and tricks to acquire the images of each character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the necessary paths\n",
    "path = Path('tng_characters')\n",
    "classes = ['jean-luc-picard', 'will-riker', 'geordi-la-forge', 'worf', 'beverly-crusher', 'deanna-troi', 'data', 'tasha-yar', 'katherine-pulaski', 'wesley-crusher'] # Added\n",
    "for c in classes: # Added\n",
    "    dest = path/c\n",
    "    dest.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Taken from Jeremy Howard's Notebook*:\n",
    "\n",
    "Now you must run some Javascript code in your browser which will save the URLs of all the images you want for you dataset.  First, go to Google Images and search for the images you want per class.  Once you're on the results page, press <kbd>Ctrl</kbd><kbd>Shift</kbd><kbd>J</kbd> in Windows/Linux and <kbd>Cmd</kbd><kbd>Opt</kbd><kbd>J</kbd> in Mac, and a small window the javascript 'Console' will appear. That is where you will paste the JavaScript commands.\n",
    "\n",
    "You will need to get the urls of each of the images. Before running the following commands, you may want to disable ad blocking extensions (uBlock, AdBlockPlus etc.) in Chrome. Otherwise the window.open() command doesn't work. Then you can run the following commands:\n",
    "\n",
    "```javascript\n",
    "urls = Array.from(document.querySelectorAll('.rg_di .rg_meta')).map(el=>JSON.parse(el.textContent).ou);\n",
    "window.open('data:text/csv;charset=utf-8,' + escape(urls.join('\\n')));\n",
    "```\n",
    "\n",
    "This will generate a CSV file that contains the direct links to each image that show up on the Google image search.  Make sure you name the CSV file exactly corresponding to how each character's name is spelt in the `classes` list and save them all in the directory defined by `path` above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For completeness, they are included in this repo and in the same directory as this notebook.  We will now download the images specified by the URLs defined in each CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in classes:\n",
    "    dest = path/c\n",
    "    s = c + '.csv'\n",
    "    print('Dest: {} - File: {}'.format(dest, s))\n",
    "    download_images(path/s, dest, max_pics=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, let's go through the images and delete any we can't open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in classes:\n",
    "    verify_images(path/c, delete=True, max_size=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've also gone through the downloaded images and removed any irrelevant ones.  Now let's zip it up so we can proceed to the next part and start training.  Training will be done on Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/ray/Programming/togetherml-homework/week2/prep_work/tng_dataset.zip'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "shutil.make_archive('tng_dataset', 'zip', path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
